{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import mujoco_py\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buf = deque()\n",
    "        \n",
    "    def add(self, state, action, reward, done, state2):\n",
    "        # Adds a given experience to the replay buffer\n",
    "        \n",
    "        exp = (state, action, reward, done, state2)\n",
    "        \n",
    "        if self.count >= self.buf_size:\n",
    "            self.buf.popleft()\n",
    "            \n",
    "        self.buf.append(exp)\n",
    "        \n",
    "    def size(self):\n",
    "        return self.count\n",
    "    \n",
    "    def sample(self, size):\n",
    "        batch = []\n",
    "        \n",
    "        if self.count < size:\n",
    "            batch = random.sample(self.buf, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buf, size)\n",
    "            \n",
    "        states = np.array([])\n",
    "        actions = np.array([])\n",
    "        rewards = np.array([])\n",
    "        dones = np.array([])\n",
    "        state2s = np.array([])\n",
    "            \n",
    "        for exp in batch:\n",
    "            states = np.append(states, exp[0])\n",
    "            actions = np.append(states, exp[1])\n",
    "            rewards = np.append(states, exp[2])\n",
    "            dones = np.append(states, exp[3])\n",
    "            state2s = np.append(states, exp[4])\n",
    "            \n",
    "        return states, actions, rewards, dones, state2s\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buf.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    A tensorflow deep neural network which will determine the actions for the agent to take, given only state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, action_bound, learning_rate, p, batch_size):\n",
    "        \n",
    "        # Initalize Hyperparameters from input\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p = p\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Main Actor\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor()\n",
    "        self.network_params = tf.compat.v1.trainable_variables()\n",
    "        \n",
    "        # Target Actor\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor()\n",
    "        self.target_network_params = tf.compat.v1.trainable_variables()[len(self.network_params):]\n",
    "        \n",
    "        \n",
    "        # TFLearn Operation for updating our target network during training\n",
    "        # self.update_target_network_params = \\\n",
    "            \n",
    "        \n",
    "        # Get the gradient of the actions, this will be used to optimize our loss\n",
    "        self.action_grad = tf.compat.v1.placeholder(tf.float32, [None, self.action_dim])\n",
    "        \n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_grad)\n",
    "        self.actor_gradients = list(map(lambda x: tf.divide(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Initialize the optimization\n",
    "        self.optimize = tf.compat.v1.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "        \n",
    "    def create_actor(self):\n",
    "        # Use the tflearn API to create a neural network as described in the DDPG formulation\n",
    "        \n",
    "        # Get an inputs object\n",
    "        inputs = tflearn.input_data(shape=[self.state_dim, None])\n",
    "        \n",
    "        # Layer1\n",
    "        net = tflearn.fully_connected(inputs, 256)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        \n",
    "        # Layer2\n",
    "        net = tflearn.fully_connected(net, 256)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        \n",
    "        # Initialize our weights to random values and define our output network\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.action_dim, activation='tanh', weights_init=w_init)      \n",
    "        \n",
    "        # Return a scaled output which is between the action bounds\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        \n",
    "        return inputs, out, scaled_out\n",
    "    \n",
    "    \n",
    "    def train(self, inputs, a_gradient):\n",
    "        # Train the created neural network with given inputs and gradient        \n",
    "        self.optimize(inputs, a_gradient)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Predict the best action for the given state using the main actor\n",
    "        model = tflearn.DNN(self.scaled_out)\n",
    "     \n",
    "        return model.predict(inputs)\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        # Predict the best action for the given state, using the target actor\n",
    "        model = tflearn.DNN(self.target_scaled_out)\n",
    "        return model.predict(inputs)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # Call the already create tflearn Operation to update the target network\n",
    "        [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.p) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.p))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    A tensorflow deep neural network which will determine the Q value for a given state and the best action from the actor network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, learning_rate, p, gamma, num_actor_vars):\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Main Critic\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.compat.v1.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Critic\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.compat.v1.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "        \n",
    "        # Tensorflow Operation for updating the target network\n",
    "        # self.update_target_network_params = \\\n",
    "            \n",
    "\n",
    "        self.predicted_q_value = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.compat.v1.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient actions\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        \n",
    "        # Layer1\n",
    "        net = tflearn.fully_connected(inputs, 256)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "\n",
    "        # Layer2\n",
    "        t1 = tflearn.fully_connected(net, 256)\n",
    "        t2 = tflearn.fully_connected(action, 256)\n",
    "\n",
    "        net = tflearn.activation(\n",
    "            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # init our weights to random values and finish with a single linear activation function\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        # Train this network on a given set of inputs, action, and a predicted value from the target network\n",
    "        \n",
    "        out = self.out(inputs, action)\n",
    "        optimized = self.optimize(predicted_q_value)\n",
    "        \n",
    "        return out, optimized\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        # Predict based on the main critic\n",
    "        model = tflearn.DNN(self.out)\n",
    "        return model.predict(inputs, actions)\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        # Predict based on the target critic\n",
    "        model = tflearn.DNN(self.target_out)\n",
    "        return model.predict(inputs, action)\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        # Calculate action gradients\n",
    "        return self.action_grads(inputs, actions)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # Update the target network using the tflearn Operation\n",
    "        [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.p) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.p))\n",
    "                for i in range(len(self.target_network_params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, actor, critic):\n",
    "    \n",
    "    # Init constants\n",
    "    BUFFER_SIZE = 10 ** 6\n",
    "    NUM_EPISODES = 19000\n",
    "    NUM_EPOCH = 50\n",
    "    EPOCH_SIZE = NUM_EPISODES / NUM_EPOCH\n",
    "    CYCLE_RATE = 50\n",
    "    NUM_BATCHES = 40\n",
    "    BATCH_SIZE = 256\n",
    "    CYCLE_SIZE = EPOCH_SIZE / CYCLE_RATE\n",
    "    BATCH_SIZE = 256\n",
    "    TIME = 50\n",
    "    NOISE_SCALE = .2\n",
    "    \n",
    "    \n",
    "    tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "    \n",
    "    rb = ReplayBuffer(BUFFER_SIZE)\n",
    "    \n",
    "    \n",
    "    for episode in range(NUM_EPISODES): \n",
    "        state = env.reset()\n",
    "        state = flatten_state(state)\n",
    "        episode_reward = 0\n",
    "        mean_max_q = 0\n",
    "        \n",
    "        for t in range(TIME):\n",
    "            action = actor.predict(np.reshape(state, (1, actor.state_dim))) + np.random.rand((1, actor.state_dim)) * NOISE_SCALE\n",
    "            \n",
    "            state2, reward, done, info = env.step(action)\n",
    "            state2 = flatten_state(state2)\n",
    "            \n",
    "            rb.add(state, action, reward, done, state2)\n",
    "            \n",
    "            if done:\n",
    "                print(f'Completed Episode {episode}: Max-Q Value Avg : {mean_max_q}, Reward: {episode_reward}')\n",
    "                break\n",
    "        \n",
    "        if episode % CYCLE_SIZE and rb.count >= BATCH_SIZE:\n",
    "            for batch_idx in range(BATCH_NUM):\n",
    "                \n",
    "                states, actions, rewards, dones, state2s = rb.sample(BATCH_SIZE)\n",
    "                \n",
    "                target_q = critic.predict_target(state2s, actor.predict_target(state2s))\n",
    "                \n",
    "                y = []\n",
    "                for k in range(BATCH_SIZE):\n",
    "                    if dones[k]:\n",
    "                        y.append(rewards[k])\n",
    "                    else:\n",
    "                        y.append(rewards[k] * critic.gamma * target_q[k])\n",
    "                        \n",
    "                q_values, opt = critic.train(states, actions, np.reshape(y, (BATCH_SIZE, 1)))\n",
    "                \n",
    "                mean_max_q += np.amax(q_values)\n",
    "                \n",
    "                action_pred = actor.predict(states)\n",
    "                grads = critic.action_gradients(states, action_pred)\n",
    "                actor.train(states, grads[0])\n",
    "                \n",
    "                actor.update_target_network()\n",
    "                critic.update_target_network()\n",
    "                \n",
    "        state = state2\n",
    "        episode_reward += reward\n",
    "        \n",
    "        \n",
    "def flatten_state(state):\n",
    "    out_state = np.array([])\n",
    "    \n",
    "    out_state = np.append(out_state, state['observation'])\n",
    "    out_state = np.append(out_state, state['achieved_goal'])\n",
    "    out_state = np.append(out_state, state['desired_goal'])\n",
    "    \n",
    "    return out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-dde29a126ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0maction_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActorNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mcritic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCriticNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_trainable_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-7af60e2efee9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, state_dim, action_dim, action_bound, learning_rate, p, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Main Actor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaled_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-7af60e2efee9>\u001b[0m in \u001b[0;36mcreate_actor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Layer1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tflearn\\layers\\core.py\u001b[0m in \u001b[0;36mfully_connected\u001b[1;34m(incoming, n_units, activation, bias, weights_init, bias_init, regularizer, weight_decay, trainable, restore, reuse, scope, name)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_incoming_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Incoming Tensor shape must be at least 2-D\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mn_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     with tf.variable_scope(scope, default_name=name, values=[incoming],\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2998\u001b[0m     \"\"\"\n\u001b[0;32m   2999\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[1;32m-> 3000\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   3001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "env = gym.make('FetchSlide-v1')\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "state_dim = env.observation_space['observation'].shape[0] + 6\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = 1\n",
    "    \n",
    "actor = ActorNetwork(state_dim, action_dim, 1, .0001, .95, 256)\n",
    "critic = CriticNetwork(state_dim, action_dim, .0001, .95, 1, actor.get_num_trainable_vars())\n",
    "    \n",
    "train(env, actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 31)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.zeros(31)\n",
    "\n",
    "inputs = inputs.reshape((1, inputs.size))\n",
    "inputs = np.tile(inputs, (2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
